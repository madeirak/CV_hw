{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0ee11ee60b2f1d5a5c0f6ad03134277d68460934a104cd1bba4a0e1d0a5fa9dd9",
   "display_name": "Python 3.8.5 64-bit ('cv_hw': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project V. Fish Detection with Deep Learning\n",
    "1. Split Train and Val dataset\n",
    "2. Train a detection model based on YOLOv3-tiny\n",
    "3. Evaluate your model\n",
    "4. Use your model to detect fish from images in data/samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup\n",
    "Please install required packages and make sure the version are valid \n",
    "\n",
    "pip install -r requirements.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from utils.logger import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "from utils.augmentations import *\n",
    "from utils.transforms import *\n",
    "from utils.parse_config import *\n",
    "from utils.test import evaluate\n",
    "from utils.loss import compute_loss\n",
    "from utils.models import *\n",
    "\n",
    "from terminaltables import AsciiTable\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "source": [
    "# Data Preprocess\n",
    "You should code this part first"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You should generate valid Train dataset and Val dataset.\n",
    "# Use data in data/custom/images and data/custom/labels to generate the path file train.txt and \n",
    "# valid.txt in data/custom/\n",
    "# a qualified val dataset is smaller than the train dataset and \n",
    "# most time there are no overlapped data between two sets.\n",
    "# train.txt & valid.txt: one image per line\n",
    "images_list = glob(os.path.join('.', 'data', 'custom', 'images', '*.jpg'))\n",
    "train_txt_path = os.path.join('.', 'data', 'custom', 'train.txt')\n",
    "val_txt_path = os.path.join('.', 'data', 'custom', 'valid.txt')\n",
    "\n",
    "num_images = len(images_list)\n",
    "train_set_size = int(num_images*0.6)\n",
    "val_set_size = num_images - train_set_size\n",
    "\n",
    "train_images_list = images_list[:train_set_size]\n",
    "val_images_list = images_list[train_set_size:]\n",
    "\n",
    "if os.path.isfile(train_txt_path):\n",
    "    os.remove(train_txt_path)\n",
    "if os.path.isfile(val_txt_path):\n",
    "    os.remove(val_txt_path)\n",
    "\n",
    "# im[2:] is for remove '.\\' at the beginning, to match the format of given case\n",
    "with open(train_txt_path,'w') as f:\n",
    "    for im in train_images_list:\n",
    "        f.write(f'{im[2:]}\\n')\n",
    "\n",
    "with open(val_txt_path,'w') as f:\n",
    "    for im in val_images_list:\n",
    "        f.write(f'{im[2:]}\\n')\n",
    "\n"
   ]
  },
  {
   "source": [
    "Make some config..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/custom/train.txt\ndata/custom/valid.txt\n['Fish']\n"
     ]
    }
   ],
   "source": [
    "opt = {\n",
    "    \"epochs\": 60,\n",
    "    \"model_def\": \"config/yolov3-tiny.cfg\",\n",
    "    \"data_config\": \"config/custom.data\",\n",
    "    \"pretrained_weights\": \"\",\n",
    "    \"n_cpu\": 1,\n",
    "    \"img_size\": 416,\n",
    "    \"multiscale_training\": True,\n",
    "    \"detect_image_folder\": \"data/samples\"\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)  \n",
    "\n",
    "# Get data configuration    \n",
    "data_config = parse_data_config(opt[\"data_config\"])    \n",
    "train_path = data_config[\"train\"]    \n",
    "valid_path = data_config[\"valid\"]    \n",
    "class_names = load_classes(data_config[\"names\"])\n",
    "print(train_path)\n",
    "print(valid_path)\n",
    "print(class_names)"
   ]
  },
  {
   "source": [
    "use pytorch to generate our model and dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Initiate model\n",
    "model = Darknet(opt[\"model_def\"]).to(device)\n",
    "model.apply(weights_init_normal)\n",
    "\n",
    "# If specified we start from checkpoint\n",
    "if opt[\"pretrained_weights\"] != \"\":\n",
    "    if opt[\"pretrained_weights\"].endswith(\".pth\"):\n",
    "         model.load_state_dict(torch.load(opt[\"pretrained_weights\"]))\n",
    "    else:\n",
    "         model.load_darknet_weights(opt[\"pretrained_weights\"])\n",
    "\n",
    "# Get dataloader\n",
    "dataset = ListDataset(train_path, multiscale=opt[\"multiscale_training\"], img_size=opt[\"img_size\"], transform=AUGMENTATION_TRANSFORMS)\n",
    "print(len(dataset.__getitem__(0)))\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size= model.hyperparams['batch'] // model.hyperparams['subdivisions'],\n",
    "    shuffle=True,\n",
    "    # num_workers=opt[\"n_cpu\"],\n",
    "    pin_memory=True,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n",
    "\n",
    "if (model.hyperparams['optimizer'] in [None, \"adam\"]):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=model.hyperparams['learning_rate'],\n",
    "        weight_decay=model.hyperparams['decay'],\n",
    "        )\n",
    "elif (model.hyperparams['optimizer'] == \"sgd\"):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=model.hyperparams['learning_rate'],\n",
    "        weight_decay=model.hyperparams['decay'],\n",
    "        momentum=model.hyperparams['momentum'])\n",
    "else:\n",
    "    print(\"Unknown optimizer. Please choose between (adam, sgd).\")\n"
   ]
  },
  {
   "source": [
    "# Train your model!\n",
    "You are required to complete the DL project training steps (get data batch from dataloader, forward, compute the loss and backward)\n",
    "see more details in following comments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "---- Training Model ----\n",
      "Epoch:1, Step1/72, loss:5.595037460327148\n",
      "Epoch:1, Step2/72, loss:5.04945707321167\n",
      "Epoch:1, Step3/72, loss:4.449802398681641\n",
      "Epoch:1, Step4/72, loss:3.743774890899658\n",
      "Epoch:1, Step5/72, loss:3.350261688232422\n",
      "Epoch:1, Step6/72, loss:2.8249576091766357\n",
      "Epoch:1, Step7/72, loss:2.871312141418457\n",
      "Epoch:1, Step8/72, loss:2.1899731159210205\n",
      "Epoch:1, Step9/72, loss:2.1420326232910156\n",
      "Epoch:1, Step10/72, loss:1.7317065000534058\n",
      "Epoch:1, Step11/72, loss:1.6434321403503418\n",
      "Epoch:1, Step12/72, loss:1.4428608417510986\n",
      "Epoch:1, Step13/72, loss:1.4120488166809082\n",
      "Epoch:1, Step14/72, loss:1.8092010021209717\n",
      "Epoch:1, Step15/72, loss:1.0434226989746094\n",
      "Epoch:1, Step16/72, loss:0.9525754451751709\n",
      "Epoch:1, Step17/72, loss:0.8846005797386169\n",
      "Epoch:1, Step18/72, loss:0.829906165599823\n",
      "Epoch:1, Step19/72, loss:0.6901243329048157\n",
      "Epoch:1, Step20/72, loss:0.7900049090385437\n",
      "Epoch:1, Step21/72, loss:0.7584354281425476\n",
      "Epoch:1, Step22/72, loss:0.9188603162765503\n",
      "Epoch:1, Step23/72, loss:0.7286558747291565\n",
      "Epoch:1, Step24/72, loss:0.5085138082504272\n",
      "Epoch:1, Step25/72, loss:0.5711745619773865\n",
      "Epoch:1, Step26/72, loss:0.9163445234298706\n",
      "Epoch:1, Step27/72, loss:0.6577978730201721\n",
      "Epoch:1, Step28/72, loss:0.523624837398529\n",
      "Epoch:1, Step29/72, loss:0.3914925456047058\n",
      "Epoch:1, Step30/72, loss:0.5653905868530273\n",
      "Epoch:1, Step31/72, loss:0.5770090818405151\n",
      "Epoch:1, Step32/72, loss:0.5095521807670593\n",
      "Epoch:1, Step33/72, loss:0.5330248475074768\n",
      "Epoch:1, Step34/72, loss:0.44283515214920044\n",
      "Epoch:1, Step35/72, loss:0.5030704736709595\n",
      "Epoch:1, Step36/72, loss:0.4817672371864319\n",
      "Epoch:1, Step37/72, loss:0.5132144689559937\n",
      "Epoch:1, Step38/72, loss:0.4108012616634369\n",
      "Epoch:1, Step39/72, loss:0.4244137704372406\n",
      "Epoch:1, Step40/72, loss:0.401382178068161\n",
      "Epoch:1, Step41/72, loss:0.5327814221382141\n",
      "Epoch:1, Step42/72, loss:0.3754733204841614\n",
      "Epoch:1, Step43/72, loss:0.3993401825428009\n",
      "Epoch:1, Step44/72, loss:0.3258446156978607\n",
      "Epoch:1, Step45/72, loss:0.4494186341762543\n",
      "Epoch:1, Step46/72, loss:0.5101770758628845\n",
      "Epoch:1, Step47/72, loss:0.48726147413253784\n",
      "Epoch:1, Step48/72, loss:0.33600929379463196\n",
      "Epoch:1, Step49/72, loss:0.3208896517753601\n",
      "Epoch:1, Step50/72, loss:0.28059834241867065\n",
      "Epoch:1, Step51/72, loss:0.35329893231391907\n",
      "Epoch:1, Step52/72, loss:0.3059336245059967\n",
      "Epoch:1, Step53/72, loss:0.27435895800590515\n",
      "Epoch:1, Step54/72, loss:0.3209073841571808\n",
      "Epoch:1, Step55/72, loss:0.3011491000652313\n",
      "Epoch:1, Step56/72, loss:0.47066187858581543\n",
      "Epoch:1, Step57/72, loss:0.25927072763442993\n",
      "Epoch:1, Step58/72, loss:0.3780035674571991\n",
      "Epoch:1, Step59/72, loss:0.25219738483428955\n",
      "Epoch:1, Step60/72, loss:0.24058911204338074\n",
      "Epoch:1, Step61/72, loss:0.3639185428619385\n",
      "Epoch:1, Step62/72, loss:0.35437506437301636\n",
      "Epoch:1, Step63/72, loss:0.24255989491939545\n",
      "Epoch:1, Step64/72, loss:0.24924305081367493\n",
      "Epoch:1, Step65/72, loss:0.3721640706062317\n",
      "Epoch:1, Step66/72, loss:0.29514434933662415\n",
      "Epoch:1, Step67/72, loss:0.22882872819900513\n",
      "Epoch:1, Step68/72, loss:0.3099042475223541\n",
      "Epoch:1, Step69/72, loss:0.30994248390197754\n",
      "Epoch:1, Step70/72, loss:0.24311646819114685\n",
      "Epoch:1, Step71/72, loss:0.19706034660339355\n",
      "Epoch:1, Step72/72, loss:0.28587499260902405\n",
      "\n",
      "---- Training Model ----\n",
      "Epoch:2, Step1/72, loss:0.3440505564212799\n",
      "Epoch:2, Step2/72, loss:0.18804584443569183\n",
      "Epoch:2, Step3/72, loss:0.2942889928817749\n",
      "Epoch:2, Step4/72, loss:0.3218524158000946\n",
      "Epoch:2, Step5/72, loss:0.24375979602336884\n",
      "Epoch:2, Step6/72, loss:0.2174922525882721\n",
      "Epoch:2, Step7/72, loss:0.21213501691818237\n",
      "Epoch:2, Step8/72, loss:0.35374516248703003\n",
      "Epoch:2, Step9/72, loss:0.4095919132232666\n",
      "Epoch:2, Step10/72, loss:0.26027712225914\n",
      "Epoch:2, Step11/72, loss:0.2925804555416107\n",
      "Epoch:2, Step12/72, loss:0.3648894429206848\n",
      "Epoch:2, Step13/72, loss:0.2257419377565384\n",
      "Epoch:2, Step14/72, loss:0.20744945108890533\n",
      "Epoch:2, Step15/72, loss:0.27647486329078674\n",
      "Epoch:2, Step16/72, loss:0.2457420378923416\n",
      "Epoch:2, Step17/72, loss:0.18324147164821625\n",
      "Epoch:2, Step18/72, loss:0.29322969913482666\n",
      "Epoch:2, Step19/72, loss:0.2776615023612976\n",
      "Epoch:2, Step20/72, loss:0.2300545871257782\n",
      "Epoch:2, Step21/72, loss:0.48871272802352905\n",
      "Epoch:2, Step22/72, loss:0.6021344661712646\n",
      "Epoch:2, Step23/72, loss:0.4302302598953247\n",
      "Epoch:2, Step24/72, loss:0.5402795076370239\n",
      "Epoch:2, Step25/72, loss:0.6125019788742065\n",
      "Epoch:2, Step26/72, loss:0.48732298612594604\n",
      "Epoch:2, Step27/72, loss:0.4646686911582947\n",
      "Epoch:2, Step28/72, loss:0.2763814926147461\n",
      "Epoch:2, Step29/72, loss:0.2789963185787201\n",
      "Epoch:2, Step30/72, loss:0.6116785407066345\n",
      "Epoch:2, Step31/72, loss:0.3384275436401367\n",
      "Epoch:2, Step32/72, loss:0.4512411057949066\n",
      "Epoch:2, Step33/72, loss:0.3532862961292267\n",
      "Epoch:2, Step34/72, loss:0.46819645166397095\n",
      "Epoch:2, Step35/72, loss:0.4896192252635956\n",
      "Epoch:2, Step36/72, loss:0.5223559737205505\n",
      "Epoch:2, Step37/72, loss:0.30543333292007446\n",
      "Epoch:2, Step38/72, loss:0.22091986238956451\n",
      "Epoch:2, Step39/72, loss:0.43460914492607117\n",
      "Epoch:2, Step40/72, loss:0.33906593918800354\n",
      "Epoch:2, Step41/72, loss:0.45946821570396423\n",
      "Epoch:2, Step42/72, loss:0.5006884336471558\n",
      "Epoch:2, Step43/72, loss:0.2851158082485199\n",
      "Epoch:2, Step44/72, loss:0.3984118700027466\n",
      "Epoch:2, Step45/72, loss:0.43395692110061646\n",
      "Epoch:2, Step46/72, loss:0.39661651849746704\n",
      "Epoch:2, Step47/72, loss:0.49243515729904175\n",
      "Epoch:2, Step48/72, loss:0.347194105386734\n",
      "Epoch:2, Step49/72, loss:0.3508187234401703\n",
      "Epoch:2, Step50/72, loss:0.39341625571250916\n",
      "Epoch:2, Step51/72, loss:0.22821560502052307\n",
      "Epoch:2, Step52/72, loss:0.30788135528564453\n",
      "Epoch:2, Step53/72, loss:0.18447443842887878\n",
      "Epoch:2, Step54/72, loss:0.2003423273563385\n",
      "Epoch:2, Step55/72, loss:0.3114458918571472\n",
      "Epoch:2, Step56/72, loss:0.32761648297309875\n",
      "Epoch:2, Step57/72, loss:0.3712630271911621\n",
      "Epoch:2, Step58/72, loss:0.3370669186115265\n",
      "Epoch:2, Step59/72, loss:0.2414424866437912\n",
      "Epoch:2, Step60/72, loss:0.3353445827960968\n",
      "Epoch:2, Step61/72, loss:0.19427788257598877\n",
      "Epoch:2, Step62/72, loss:0.2669118344783783\n",
      "Epoch:2, Step63/72, loss:0.25392988324165344\n",
      "Epoch:2, Step64/72, loss:0.2271387279033661\n",
      "Epoch:2, Step65/72, loss:0.2691572308540344\n",
      "Epoch:2, Step66/72, loss:0.26466986536979675\n",
      "Epoch:2, Step67/72, loss:0.21672967076301575\n",
      "Epoch:2, Step68/72, loss:0.24539607763290405\n",
      "Epoch:2, Step69/72, loss:0.20985694229602814\n",
      "Epoch:2, Step70/72, loss:0.17426800727844238\n",
      "Epoch:2, Step71/72, loss:0.35469794273376465\n",
      "Epoch:2, Step72/72, loss:0.35723453760147095\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt[\"epochs\"]):\n",
    "#for epoch in range(2):\n",
    "    print(\"\\n---- Training Model ----\")\n",
    "    # activate BN and dropout\n",
    "    model.train()\n",
    "\n",
    "    # Your code need to execute forward and backward steps.\n",
    "    # Use 'enumerate' to get a batch[_, images, targets]\n",
    "    # some helpful function\n",
    "    # - outputs = model.__call__(imgs)(use it by model(imgs))\n",
    "    # - loss, _ = cumpte_loss(outputs, targets, model)\n",
    "    # - loss.backward() (backward step)\n",
    "    # - optimizer.step() (execute params updating)\n",
    "    # - optimizer.zero_grad() (reset gradients)\n",
    "    # if you want to see how loss changes in each mini-batch step:\n",
    "    # -eg print(f'Epoch:{epoch+1}, Step{step+1}/{len(dataloader)}, loss:{loss.item()}')\n",
    "\n",
    "    # each epoch\n",
    "    \n",
    "    for step, (_, im, targets) in enumerate(dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            im=im.type(torch.cuda.FloatTensor)\n",
    "            targets=targets.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "        y_pred = model.forward(im)\n",
    "        loss, _ = compute_loss(y_pred, targets, model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch:{epoch+1}, Step{step+1}/{len(dataloader)}, loss:{loss.item()}')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "source": [
    "# Evaluate and save current model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Detecting objects:   0%|          | 0/49 [00:00<?, ?it/s]\n",
      "---- Evaluating Model ----\n",
      "Detecting objects: 100%|██████████| 49/49 [00:03<00:00, 13.74it/s]\n",
      "Computing AP: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]['Fish'] 0\n",
      "+-------+------------+---------+\n",
      "| Index | Class name | AP      |\n",
      "+-------+------------+---------+\n",
      "| 0     | Fish       | 0.00000 |\n",
      "+-------+------------+---------+\n",
      "---- mAP 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---- Evaluating Model ----\")\n",
    "# Evaluate the model on the validation set\n",
    "metrics_output = evaluate(\n",
    "    model,\n",
    "    path=valid_path,\n",
    "    iou_thres=0.5,\n",
    "    conf_thres=0.1,\n",
    "    nms_thres=0.5,\n",
    "    img_size=opt[\"img_size\"],\n",
    "    batch_size=model.hyperparams['batch'] // model.hyperparams['subdivisions'],\n",
    ")\n",
    "\n",
    "if metrics_output is not None:\n",
    "    precision, recall, AP, f1, ap_class = metrics_output\n",
    "    evaluation_metrics = [\n",
    "                (\"validation/precision\", precision.mean()),\n",
    "                (\"validation/recall\", recall.mean()),\n",
    "                (\"validation/mAP\", AP.mean()),\n",
    "                (\"validation/f1\", f1.mean()),\n",
    "                ]\n",
    "    # Print class APs and mAP\n",
    "    ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
    "    for i, c in enumerate(ap_class):\n",
    "        print(class_names, c)\n",
    "        ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
    "    print(AsciiTable(ap_table).table)\n",
    "    print(f\"---- mAP {AP.mean()}\")                \n",
    "else:\n",
    "    print( \"---- mAP not measured (no detections found by model)\")\n",
    "torch.save(model.state_dict(), f\"checkpoints/yolov3-tiny_ckpt_%d.pth\" % epoch)"
   ]
  },
  {
   "source": [
    "# Detect and visualize results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Performing object detection:\n",
      "\n",
      "Saving images:\n",
      "(0) Image: 'data/samples\\test (1).jpg'\n",
      "(1) Image: 'data/samples\\test (10).jpg'\n",
      "(2) Image: 'data/samples\\test (11).jpg'\n",
      "(3) Image: 'data/samples\\test (2).jpg'\n",
      "\t+ Label: Fish, Conf: 0.20867\n",
      "\t+ Label: Fish, Conf: 0.20627\n",
      "(4) Image: 'data/samples\\test (3).jpg'\n",
      "(5) Image: 'data/samples\\test (4).jpg'\n",
      "(6) Image: 'data/samples\\test (5).jpg'\n",
      "(7) Image: 'data/samples\\test (6).jpg'\n",
      "(8) Image: 'data/samples\\test (7).jpg'\n",
      "(9) Image: 'data/samples\\test (8).jpg'\n",
      "(10) Image: 'data/samples\\test (9).jpg'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "model.eval()  # Set in evaluation mode\n",
    "dataloader = DataLoader(\n",
    "        ImageFolder(opt[\"detect_image_folder\"], transform= \\\n",
    "            transforms.Compose([DEFAULT_TRANSFORMS, Resize(opt[\"img_size\"])])),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "imgs = []  # Stores image paths\n",
    "img_detections = []  # Stores detections for each image index\n",
    "print(\"\\nPerforming object detection:\")\n",
    "for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "    # Configure input\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs)\n",
    "        detections = non_max_suppression(detections, 0.2, 0.7)\n",
    "    imgs.extend(img_paths)\n",
    "    img_detections.extend(detections)\n",
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap(\"tab20b\")\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "print(\"\\nSaving images:\")\n",
    "# Iterate through images and save plot of detections\n",
    "for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "    print(\"(%d) Image: '%s'\" % (img_i, path))\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        # Rescale boxes to original image\n",
    "        detections = detections.cpu()\n",
    "        detections = rescale_boxes(detections, opt[\"img_size\"], img.shape[:2])\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "        for x1, y1, x2, y2, cls_conf, cls_pred in detections:\n",
    "            print(\"\\t+ Label: %s, Conf: %.5f\" % (class_names[int(cls_pred)], cls_conf.item()))\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(\n",
    "                x1,\n",
    "                y1,\n",
    "                s=class_names[int(cls_pred)],\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": color, \"pad\": 0},\n",
    "            )\n",
    "    # Save generated image with detections\n",
    "    plt.axis(\"off\")\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    filename = os.path.basename(path).split(\".\")[0]\n",
    "    output_path = os.path.join(\"output\", f\"{filename}.jpg\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}